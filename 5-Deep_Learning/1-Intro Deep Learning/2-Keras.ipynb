{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE** Podemos llegar a tener problemas con los Long Paths de Windows 10\n",
    "Urls clave:\n",
    "- https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=registry#enable-long-paths-in-windows-10-version-1607-and-later\n",
    "- https://support.microsoft.com/en-us/windows/how-to-open-registry-editor-in-windows-10-deab38e6-91d6-e0aa-4b7c-8878d9e07b11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MNIST dataset is commonly used as a benchmark to test image classification models. Loading it with Keras provides an easy way to get started with image recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train is a 60000 x 28 x 28 array - this contains the 60,000 28x28 pixel training images\n",
    "\n",
    "y_train is a 60000 element array - the corresponding digit labels for each training image\n",
    "\n",
    "X_test is 10000 x 28 x 28 - the 10,000 test images\n",
    "\n",
    "y_test is 10000 elements - the test labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the first sample from the X_train data\n",
    "X_train[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0], cmap= plt.colormaps[\"Greys\"]);# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train and X_test are cast to float32 datatype\n",
    "\n",
    "The pixel values are then divided by 255 to scale to 0-1 range\n",
    "\n",
    "This rescaling is commonly done as a preprocessing step for image data before feeding into a neural network model.\n",
    "\n",
    "The reasons are:\n",
    "\n",
    "Neural networks typically expect float input data for the math to work properly.\n",
    "\n",
    "Scaling to 0-1 range helps the data distribution have a similar range throughout the dataset. This can help improve model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13066062"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.mean(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the preprocessing scaled the values reasonably. Since the images were scaled to 0-1 range, we'd expect the mean to be close to 0.5.\n",
    "\n",
    "To check that the training data distribution looks sensible before feeding into the model. The mean gives a sense of the central tendency.\n",
    "\n",
    "To use for data normalization/standardization. Some models benefit from input data with a zero mean. So you could subtract the mean from each input as additional preprocessing.\n",
    "\n",
    "As a baseline for model accuracy. A simple model that predicts the mean pixel value would get around 10% accuracy on MNIST. So checking the mean gives a minimum baseline to beat.\n",
    "\n",
    "To potentially detect any anomalies or issues with the loaded data by checking if the mean is unexpectedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels in the MNIST dataset originally are integer values between 0-9 indicating the digit class.\n",
    "\n",
    "Typically for neural network classification, it is common practice to encode labels as \"one-hot\" vectors. This represents each class as a vector with a 1 in the index for that class and 0s elsewhere.\n",
    "\n",
    "Keras expects label inputs to be encoded as binary vectors when training classification models.\n",
    "\n",
    "So converting the integer labels to float32 is likely in preparation for one-hot encoding the labels before model training. This is a necessary data preprocessing step.\n",
    "\n",
    "The float32 type allows the vector elements to have fractional values (0s and 1s) compared to integers.\n",
    "\n",
    "In summary, this casting to float32 is done to get the label data ready for one-hot encoding further preprocessing before feeding into the Keras neural network model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last 10,000 images and labels from X_train and y_train are assigned to new X_val and y_val variables.\n",
    "\n",
    "X_train and y_train are then reassigned to only contain the first 60,000-10,000=50,000 examples.\n",
    "\n",
    "So the full 60,000 training examples have been split into:\n",
    "\n",
    "X_train, y_train - 50,000 examples for training the model\n",
    "X_val, y_val - 10,000 examples to validate the model during training\n",
    "Using a validation set allows monitoring model performance on data not used in training. This helps detect and prevent overfitting.\n",
    "\n",
    "The validation data is typically a subset taken from the original training data, as shown here.\n",
    "\n",
    "In summary, this code is creating new validation sets by splitting off part of the original training data. This is a good practice for model training and evaluation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbZElEQVR4nO3dX2zV9f3H8dfh3wGkPayW9vRIYS3+wQntIoPaqEyloXQJEeRC1CVADE4sTmD+SRcB2ZZ0YubP6BAu3OhcBB2ZQHQZCRRb4ihsoISgW6XYDVxpmRjOKUVKRz+/C8KZR1rgezyn7572+UhOYs857573vjv26eGcfvE555wAAOhhA6wXAAD0TwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGGS9wNd1dnaqqalJaWlp8vl81usAADxyzqm1tVWhUEgDBnT/OqfXBaipqUm5ubnWawAAvqFjx45p9OjR3d7e6wKUlpYm6cLi6enpxtsAALyKRCLKzc2N/jzvTtICtGbNGr3wwgtqbm5WYWGhXnnlFU2ZMuWKcxf/2C09PZ0AAUAKu9LbKEn5EMJbb72lZcuWaeXKlfrggw9UWFio0tJSnThxIhkPBwBIQUkJ0IsvvqiFCxdqwYIF+s53vqN169Zp+PDh+u1vf5uMhwMApKCEB+jcuXPav3+/SkpK/vcgAwaopKREdXV1l9y/vb1dkUgk5gIA6PsSHqDPP/9c58+fV3Z2dsz12dnZam5uvuT+lZWVCgQC0QufgAOA/sH8F1ErKioUDoejl2PHjlmvBADoAQn/FFxmZqYGDhyolpaWmOtbWloUDAYvub/f75ff70/0GgCAXi7hr4CGDBmiSZMmqbq6OnpdZ2enqqurVVxcnOiHAwCkqKT8HtCyZcs0b948fe9739OUKVP00ksvqa2tTQsWLEjGwwEAUlBSAnT//ffrP//5j1asWKHm5mZ997vf1bZt2y75YAIAoP/yOeec9RJfFYlEFAgEFA6HORMCAKSgq/05bv4pOABA/0SAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMDLJeALiSpqYmzzO/+tWv4nqsTz75xPNMJBLxPPPrX//a80xra6vnmUmTJnmekSS/3x/XHOAFr4AAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABOcjBQ96ty5c55nCgoKPM988cUXnmd6UmFhYY88ztixY+Oa27hxo+eZ2267La7HQv/FKyAAgAkCBAAwkfAAPffcc/L5fDGX8ePHJ/phAAApLinvAd1yyy3asWPH/x5kEG81AQBiJaUMgwYNUjAYTMa3BgD0EUl5D+jw4cMKhULKz8/XQw89pKNHj3Z73/b2dkUikZgLAKDvS3iAioqKVFVVpW3btmnt2rVqbGzUnXfe2e3fZ19ZWalAIBC95ObmJnolAEAv5HPOuWQ+wKlTpzR27Fi9+OKLevjhhy+5vb29Xe3t7dGvI5GIcnNzFQ6HlZ6enszVYCCe3wMKhUKeZ3r77wH1FH4PCBYikYgCgcAVf44n/dMBI0eO1I033qiGhoYub/f7/fL7/cleAwDQyyT994BOnz6tI0eOKCcnJ9kPBQBIIQkP0JNPPqna2lr985//1O7duzV79mwNHDhQDzzwQKIfCgCQwhL+R3CfffaZHnjgAZ08eVKjRo3SHXfcoT179mjUqFGJfigAQApL+ocQvLraN6+Qmr76gZOrNWfOHM8zWVlZnmckafLkyZ5n/va3v3me+eSTTzzPHD582PNMW1ub5xlJuu666zzPfPDBB55nrrnmGs8z6P2u9uc454IDAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwMlIgRZw5c8bzzKuvvhrXYz399NOeZ3bs2OF55p577vE8g96Pk5ECAHo1AgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmBhkvQCAqzN8+HDPM3fffXcSNuna7t27Pc9wNuz+jVdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJTkYKpIizZ896nqmoqEjCJl3797//3WOPhb6BV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAlORgoYaGpq8jxTWlrqeeajjz7yPCNJBQUFnmeef/75uB4L/RevgAAAJggQAMCE5wDt2rVLM2fOVCgUks/n05YtW2Jud85pxYoVysnJ0bBhw1RSUqLDhw8nal8AQB/hOUBtbW0qLCzUmjVrurx99erVevnll7Vu3Trt3btX11xzjUpLS+P6y7QAAH2X5w8hlJWVqaysrMvbnHN66aWX9Oyzz+ree++VJL3++uvKzs7Wli1bNHfu3G+2LQCgz0joe0CNjY1qbm5WSUlJ9LpAIKCioiLV1dV1OdPe3q5IJBJzAQD0fQkNUHNzsyQpOzs75vrs7OzobV9XWVmpQCAQveTm5iZyJQBAL2X+KbiKigqFw+Ho5dixY9YrAQB6QEIDFAwGJUktLS0x17e0tERv+zq/36/09PSYCwCg70togPLy8hQMBlVdXR29LhKJaO/evSouLk7kQwEAUpznT8GdPn1aDQ0N0a8bGxt14MABZWRkaMyYMVqyZIl+8Ytf6IYbblBeXp6WL1+uUCikWbNmJXJvAECK8xygffv26e67745+vWzZMknSvHnzVFVVpaefflptbW165JFHdOrUKd1xxx3atm2bhg4dmritAQApz+ecc9ZLfFUkElEgEFA4HOb9IKSEnTt3ep5ZuHCh55nGxkbPM8OHD/c8I134D02vxo8fH9djoe+52p/j5p+CAwD0TwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDh+a9jAFLBuXPn4pp77bXXPM/8+Mc/9jzT2dnpeWbUqFGeZw4dOuR5Jt7HArziFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKTkaJPWrx4cVxz8ZyMNB4/+tGPPM889dRTnmc4qSh6M14BAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmOBkp+qSPP/7YeoXLeuKJJzzP5OfnJ2ETwA6vgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE5yMFH3SrFmz4prbvXt3YhfpxsSJEz3PLF++3PPM0qVLPc9IUlpaWlxzgBe8AgIAmCBAAAATngO0a9cuzZw5U6FQSD6fT1u2bIm5ff78+fL5fDGXGTNmJGpfAEAf4TlAbW1tKiws1Jo1a7q9z4wZM3T8+PHoZePGjd9oSQBA3+P5QwhlZWUqKyu77H38fr+CwWDcSwEA+r6kvAdUU1OjrKws3XTTTVq0aJFOnjzZ7X3b29sViURiLgCAvi/hAZoxY4Zef/11VVdX6/nnn1dtba3Kysp0/vz5Lu9fWVmpQCAQveTm5iZ6JQBAL5Tw3wOaO3du9J8nTpyogoICjRs3TjU1NZo2bdol96+oqNCyZcuiX0ciESIEAP1A0j+GnZ+fr8zMTDU0NHR5u9/vV3p6eswFAND3JT1An332mU6ePKmcnJxkPxQAIIV4/iO406dPx7yaaWxs1IEDB5SRkaGMjAytWrVKc+bMUTAY1JEjR/T000/r+uuvV2lpaUIXBwCkNs8B2rdvn+6+++7o1xffv5k3b57Wrl2rgwcP6ne/+51OnTqlUCik6dOn6+c//7n8fn/itgYApDyfc85ZL/FVkUhEgUBA4XCY94MQt46OjrjmHnvsMc8z7733nueZTz/91PNMPEaPHh3X3J/+9CfPM/GcYBV909X+HOdccAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDB2bCBr/jvf//reaazs9PzzNChQz3P9KRhw4Z5nnnzzTc9z8ycOdPzDHo/zoYNAOjVCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATnIwUMNDU1OR55oknnvA888c//tHzTLzy8/M9zzQ0NCRhE1jjZKQAgF6NAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADAxyHoB9C8dHR2eZwYPHpyETWyFQiHPM7///e89z8R7Qt/169d7nvn00089z5w+fdrzzIgRIzzPoHfiFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKTkSJuX3zxheeZgoICzzMPPfSQ55lbb73V84wkjR492vPMK6+84nkmnpOyfvLJJ55nPvroI88z8br55ps9z3Bi0f6NV0AAABMECABgwlOAKisrNXnyZKWlpSkrK0uzZs1SfX19zH3Onj2r8vJyXXvttRoxYoTmzJmjlpaWhC4NAEh9ngJUW1ur8vJy7dmzR9u3b1dHR4emT5+utra26H2WLl2qd955R5s2bVJtba2ampp03333JXxxAEBq8/QhhG3btsV8XVVVpaysLO3fv19Tp05VOBzWb37zG23YsEH33HOPpAt/s+LNN9+sPXv26Lbbbkvc5gCAlPaN3gMKh8OSpIyMDEnS/v371dHRoZKSkuh9xo8frzFjxqiurq7L79He3q5IJBJzAQD0fXEHqLOzU0uWLNHtt9+uCRMmSJKam5s1ZMgQjRw5Mua+2dnZam5u7vL7VFZWKhAIRC+5ubnxrgQASCFxB6i8vFyHDh3Sm2+++Y0WqKioUDgcjl6OHTv2jb4fACA1xPWLqIsXL9a7776rXbt2xfziXjAY1Llz53Tq1KmYV0EtLS0KBoNdfi+/3y+/3x/PGgCAFObpFZBzTosXL9bmzZu1c+dO5eXlxdw+adIkDR48WNXV1dHr6uvrdfToURUXFydmYwBAn+DpFVB5ebk2bNigrVu3Ki0tLfq+TiAQ0LBhwxQIBPTwww9r2bJlysjIUHp6uh5//HEVFxfzCTgAQAxPAVq7dq0k6a677oq5fv369Zo/f74k6f/+7/80YMAAzZkzR+3t7SotLdWrr76akGUBAH2HzznnrJf4qkgkokAgoHA4rPT0dOt1cBlVVVWeZxYsWOB5xufzeZ7p7eL5164nj0M8/+5196sWlxPPCUzR+13tz3HOBQcAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATcf2NqIAknThxwnqFfmXhwoWeZ1auXBnXY8VzNuwRI0bE9Vjov3gFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY8DnnnPUSXxWJRBQIBBQOh+M6ISJ6zvnz5z3PfPzxx55nXnvtNc8zTU1NnmckKRAIxDXn1ZNPPul55sYbb/Q8M2AA/42Jnne1P8d5dgIATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJjgZKQAgoTgZKQCgVyNAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmPAWosrJSkydPVlpamrKysjRr1izV19fH3Oeuu+6Sz+eLuTz66KMJXRoAkPo8Bai2tlbl5eXas2ePtm/fro6ODk2fPl1tbW0x91u4cKGOHz8evaxevTqhSwMAUt8gL3fetm1bzNdVVVXKysrS/v37NXXq1Oj1w4cPVzAYTMyGAIA+6Ru9BxQOhyVJGRkZMde/8cYbyszM1IQJE1RRUaEzZ850+z3a29sViURiLgCAvs/TK6Cv6uzs1JIlS3T77bdrwoQJ0esffPBBjR07VqFQSAcPHtQzzzyj+vp6vf32211+n8rKSq1atSreNQAAKcrnnHPxDC5atEh//vOf9f7772v06NHd3m/nzp2aNm2aGhoaNG7cuEtub29vV3t7e/TrSCSi3NxchcNhpaenx7MaAMBQJBJRIBC44s/xuF4BLV68WO+++6527dp12fhIUlFRkSR1GyC/3y+/3x/PGgCAFOYpQM45Pf7449q8ebNqamqUl5d3xZkDBw5IknJycuJaEADQN3kKUHl5uTZs2KCtW7cqLS1Nzc3NkqRAIKBhw4bpyJEj2rBhg37wgx/o2muv1cGDB7V06VJNnTpVBQUFSfkfAABITZ7eA/L5fF1ev379es2fP1/Hjh3TD3/4Qx06dEhtbW3Kzc3V7Nmz9eyzz171+zlX+2eHAIDeKSnvAV2pVbm5uaqtrfXyLQEA/RTnggMAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmBhkvcDXOeckSZFIxHgTAEA8Lv78vvjzvDu9LkCtra2SpNzcXONNAADfRGtrqwKBQLe3+9yVEtXDOjs71dTUpLS0NPl8vpjbIpGIcnNzdezYMaWnpxttaI/jcAHH4QKOwwUchwt6w3Fwzqm1tVWhUEgDBnT/Tk+vewU0YMAAjR49+rL3SU9P79dPsIs4DhdwHC7gOFzAcbjA+jhc7pXPRXwIAQBgggABAEykVID8fr9Wrlwpv99vvYopjsMFHIcLOA4XcBwuSKXj0Os+hAAA6B9S6hUQAKDvIEAAABMECABgggABAEykTIDWrFmjb3/72xo6dKiKior017/+1XqlHvfcc8/J5/PFXMaPH2+9VtLt2rVLM2fOVCgUks/n05YtW2Jud85pxYoVysnJ0bBhw1RSUqLDhw/bLJtEVzoO8+fPv+T5MWPGDJtlk6SyslKTJ09WWlqasrKyNGvWLNXX18fc5+zZsyovL9e1116rESNGaM6cOWppaTHaODmu5jjcddddlzwfHn30UaONu5YSAXrrrbe0bNkyrVy5Uh988IEKCwtVWlqqEydOWK/W42655RYdP348enn//fetV0q6trY2FRYWas2aNV3evnr1ar388stat26d9u7dq2uuuUalpaU6e/ZsD2+aXFc6DpI0Y8aMmOfHxo0be3DD5KutrVV5ebn27Nmj7du3q6OjQ9OnT1dbW1v0PkuXLtU777yjTZs2qba2Vk1NTbrvvvsMt068qzkOkrRw4cKY58Pq1auNNu6GSwFTpkxx5eXl0a/Pnz/vQqGQq6ysNNyq561cudIVFhZar2FKktu8eXP0687OThcMBt0LL7wQve7UqVPO7/e7jRs3GmzYM75+HJxzbt68ee7ee+812cfKiRMnnCRXW1vrnLvw//3gwYPdpk2bovf5+9//7iS5uro6qzWT7uvHwTnnvv/977snnnjCbqmr0OtfAZ07d0779+9XSUlJ9LoBAwaopKREdXV1hpvZOHz4sEKhkPLz8/XQQw/p6NGj1iuZamxsVHNzc8zzIxAIqKioqF8+P2pqapSVlaWbbrpJixYt0smTJ61XSqpwOCxJysjIkCTt379fHR0dMc+H8ePHa8yYMX36+fD143DRG2+8oczMTE2YMEEVFRU6c+aMxXrd6nUnI/26zz//XOfPn1d2dnbM9dnZ2frHP/5htJWNoqIiVVVV6aabbtLx48e1atUq3XnnnTp06JDS0tKs1zPR3NwsSV0+Py7e1l/MmDFD9913n/Ly8nTkyBH99Kc/VVlZmerq6jRw4EDr9RKus7NTS5Ys0e23364JEyZIuvB8GDJkiEaOHBlz3778fOjqOEjSgw8+qLFjxyoUCungwYN65plnVF9fr7fffttw21i9PkD4n7Kysug/FxQUqKioSGPHjtUf/vAHPfzww4aboTeYO3du9J8nTpyogoICjRs3TjU1NZo2bZrhZslRXl6uQ4cO9Yv3QS+nu+PwyCOPRP954sSJysnJ0bRp03TkyBGNGzeup9fsUq//I7jMzEwNHDjwkk+xtLS0KBgMGm3VO4wcOVI33nijGhoarFcxc/E5wPPjUvn5+crMzOyTz4/Fixfr3Xff1XvvvRfz17cEg0GdO3dOp06dirl/X30+dHcculJUVCRJver50OsDNGTIEE2aNEnV1dXR6zo7O1VdXa3i4mLDzeydPn1aR44cUU5OjvUqZvLy8hQMBmOeH5FIRHv37u33z4/PPvtMJ0+e7FPPD+ecFi9erM2bN2vnzp3Ky8uLuX3SpEkaPHhwzPOhvr5eR48e7VPPhysdh64cOHBAknrX88H6UxBX480333R+v99VVVW5jz/+2D3yyCNu5MiRrrm52Xq1HvWTn/zE1dTUuMbGRveXv/zFlZSUuMzMTHfixAnr1ZKqtbXVffjhh+7DDz90ktyLL77oPvzwQ/evf/3LOefcL3/5Szdy5Ei3detWd/DgQXfvvfe6vLw89+WXXxpvnliXOw6tra3uySefdHV1da6xsdHt2LHD3Xrrre6GG25wZ8+etV49YRYtWuQCgYCrqalxx48fj17OnDkTvc+jjz7qxowZ43bu3On27dvniouLXXFxseHWiXel49DQ0OB+9rOfuX379rnGxka3detWl5+f76ZOnWq8eayUCJBzzr3yyituzJgxbsiQIW7KlCluz5491iv1uPvvv9/l5OS4IUOGuOuuu87df//9rqGhwXqtpHvvvfecpEsu8+bNc85d+Cj28uXLXXZ2tvP7/W7atGmuvr7edukkuNxxOHPmjJs+fbobNWqUGzx4sBs7dqxbuHBhn/uPtK7+90ty69evj97nyy+/dI899pj71re+5YYPH+5mz57tjh8/brd0ElzpOBw9etRNnTrVZWRkOL/f766//nr31FNPuXA4bLv41/DXMQAATPT694AAAH0TAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGDi/wG3QiIBs80IOQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Deprecated! plt.imshow(X_val[0], cmap=plt.cm.get_cmap('Greys'));\n",
    "plt.imshow(X_val[0], cmap= plt.colormaps[\"Greys\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# Capa entrada\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 300,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 100,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Capa salida\n",
    "model.add(keras.layers.Dense(units = 10,\n",
    "                            activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras Sequential model architecture**\n",
    "\n",
    "\n",
    "**model** = keras.models.Sequential()\n",
    "Creates a Sequential model, which stacks layers linearly.\n",
    "\n",
    "**model.add(keras.layers.Flatten(input_shape=(28, 28)))**\n",
    "The first layer is a Flatten layer which converts the 28x28 pixel input images into 784 length (28*28) vectors to feed into the dense layers.\n",
    "\n",
    "**model.add(keras.layers.Dense(300, activation='relu'))**\n",
    "This is a fully-connected dense layer with 300 units and ReLU activation.\n",
    "\n",
    "**model.add(keras.layers.Dense(100, activation='relu'))**\n",
    "Second dense layer with 100 units and ReLU activation.\n",
    "\n",
    "**model.add(keras.layers.Dense(10, activation='softmax'))**\n",
    "The final output layer has 10 units (one per digit class) with softmax activation to output classification probabilities.\n",
    "\n",
    "*This builds a simple fully-connected neural network for classifying the handwritten digits, with two hidden layers for feature extraction before the output layer. The ReLU activation and softmax output are standard for classification models.*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266610 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The ReLU (Rectified Linear Unit) activation function** is a commonly used activation function for hidden layers in neural networks.\n",
    "\n",
    "Here's an explanation of what ReLU does:\n",
    "\n",
    "It applies the function f(x) = max(0, x) to the input.\n",
    "\n",
    "This means it thresholds all negative inputs to 0 while keeping positive inputs unchanged.\n",
    "\n",
    "The output is a simple linear function for positive values, hence the name \"rectified linear\".\n",
    "\n",
    "Some key properties and motivations for using ReLU:\n",
    "\n",
    "It introduces non-linearity into the network, allowing neural nets to learn complex relationships.\n",
    "\n",
    "Compared to other activations like sigmoid, ReLU tends to result in faster training and better generalization.\n",
    "\n",
    "It avoids the vanishing gradient problem since the gradient is either 0 or constant positive for positive inputs.\n",
    "\n",
    "It is computationally efficient as it involves simple thresholding.\n",
    "\n",
    "In summary, the ReLU activation applies element-wise nonlinearity while keeping only the positive inputs. This has become the standard activation for hidden layers due to its faster training convergence and better generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(units = 300, activation='relu'),\n",
    "    keras.layers.Dense(units = 100, activation='relu'),\n",
    "    keras.layers.Dense(units = 10, activation='softmax')\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.layers.reshaping.flatten.Flatten object at 0x143afe490>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28 = 784 # neurons   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*784 # 300 is the number of neurons in the hidden layer, 784 is the number of pixels in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size # weigh size means number of weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.SGD(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/losses/probabilistic_losses/#kldivergence-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente\n",
    "model.compile(\n",
    "    optimizer = \"sgd\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266610 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/128 = 390.625  # 128 = batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "391/391 [==============================] - 7s 13ms/step - loss: 1.2331 - accuracy: 0.6930 - val_loss: 0.5914 - val_accuracy: 0.8653\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.5121 - accuracy: 0.8686 - val_loss: 0.3952 - val_accuracy: 0.8955\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3955 - accuracy: 0.8933 - val_loss: 0.3325 - val_accuracy: 0.9096\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3457 - accuracy: 0.9037 - val_loss: 0.3001 - val_accuracy: 0.9178\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3155 - accuracy: 0.9110 - val_loss: 0.2784 - val_accuracy: 0.9222\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.2936 - accuracy: 0.9170 - val_loss: 0.2637 - val_accuracy: 0.9251\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2762 - accuracy: 0.9222 - val_loss: 0.2497 - val_accuracy: 0.9282\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2616 - accuracy: 0.9259 - val_loss: 0.2370 - val_accuracy: 0.9343\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.2491 - accuracy: 0.9292 - val_loss: 0.2294 - val_accuracy: 0.9356\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2381 - accuracy: 0.9326 - val_loss: 0.2187 - val_accuracy: 0.9386\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.2281 - accuracy: 0.9352 - val_loss: 0.2119 - val_accuracy: 0.9409\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.2191 - accuracy: 0.9378 - val_loss: 0.2027 - val_accuracy: 0.9454\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.2107 - accuracy: 0.9402 - val_loss: 0.1968 - val_accuracy: 0.9474\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.2030 - accuracy: 0.9421 - val_loss: 0.1909 - val_accuracy: 0.9485\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1960 - accuracy: 0.9442 - val_loss: 0.1849 - val_accuracy: 0.9511\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1894 - accuracy: 0.9463 - val_loss: 0.1793 - val_accuracy: 0.9516\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1830 - accuracy: 0.9486 - val_loss: 0.1748 - val_accuracy: 0.9522\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1774 - accuracy: 0.9501 - val_loss: 0.1701 - val_accuracy: 0.9538\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1716 - accuracy: 0.9516 - val_loss: 0.1677 - val_accuracy: 0.9547\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1664 - accuracy: 0.9525 - val_loss: 0.1634 - val_accuracy: 0.9557\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1615 - accuracy: 0.9544 - val_loss: 0.1590 - val_accuracy: 0.9557\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1570 - accuracy: 0.9556 - val_loss: 0.1558 - val_accuracy: 0.9573\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1523 - accuracy: 0.9574 - val_loss: 0.1537 - val_accuracy: 0.9582\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1481 - accuracy: 0.9584 - val_loss: 0.1492 - val_accuracy: 0.9582\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1440 - accuracy: 0.9598 - val_loss: 0.1472 - val_accuracy: 0.9584\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1403 - accuracy: 0.9611 - val_loss: 0.1436 - val_accuracy: 0.9614\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1366 - accuracy: 0.9619 - val_loss: 0.1414 - val_accuracy: 0.9616\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1331 - accuracy: 0.9631 - val_loss: 0.1385 - val_accuracy: 0.9617\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1298 - accuracy: 0.9638 - val_loss: 0.1362 - val_accuracy: 0.9627\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1266 - accuracy: 0.9647 - val_loss: 0.1343 - val_accuracy: 0.9633\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1235 - accuracy: 0.9658 - val_loss: 0.1316 - val_accuracy: 0.9634\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1206 - accuracy: 0.9665 - val_loss: 0.1305 - val_accuracy: 0.9647\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1179 - accuracy: 0.9676 - val_loss: 0.1272 - val_accuracy: 0.9644\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1150 - accuracy: 0.9678 - val_loss: 0.1255 - val_accuracy: 0.9660\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1125 - accuracy: 0.9688 - val_loss: 0.1243 - val_accuracy: 0.9659\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1097 - accuracy: 0.9698 - val_loss: 0.1254 - val_accuracy: 0.9660\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1074 - accuracy: 0.9703 - val_loss: 0.1228 - val_accuracy: 0.9662\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1051 - accuracy: 0.9713 - val_loss: 0.1200 - val_accuracy: 0.9673\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1027 - accuracy: 0.9718 - val_loss: 0.1186 - val_accuracy: 0.9677\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1006 - accuracy: 0.9722 - val_loss: 0.1175 - val_accuracy: 0.9680\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.0985 - accuracy: 0.9729 - val_loss: 0.1163 - val_accuracy: 0.9691\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.0966 - accuracy: 0.9736 - val_loss: 0.1131 - val_accuracy: 0.9673\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.0946 - accuracy: 0.9739 - val_loss: 0.1126 - val_accuracy: 0.9686\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.0927 - accuracy: 0.9745 - val_loss: 0.1111 - val_accuracy: 0.9693\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.0908 - accuracy: 0.9752 - val_loss: 0.1100 - val_accuracy: 0.9691\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.0890 - accuracy: 0.9758 - val_loss: 0.1104 - val_accuracy: 0.9688\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.0873 - accuracy: 0.9763 - val_loss: 0.1089 - val_accuracy: 0.9686\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.0856 - accuracy: 0.9763 - val_loss: 0.1063 - val_accuracy: 0.9703\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.0840 - accuracy: 0.9770 - val_loss: 0.1053 - val_accuracy: 0.9698\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.0824 - accuracy: 0.9776 - val_loss: 0.1052 - val_accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 50,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0818 - accuracy: 0.9779 - val_loss: 0.1057 - val_accuracy: 0.9698\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.0790 - accuracy: 0.9789 - val_loss: 0.1026 - val_accuracy: 0.9721\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.0762 - accuracy: 0.9796 - val_loss: 0.1008 - val_accuracy: 0.9712\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0734 - accuracy: 0.9804 - val_loss: 0.0986 - val_accuracy: 0.9724\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0710 - accuracy: 0.9814 - val_loss: 0.0997 - val_accuracy: 0.9705\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.0684 - accuracy: 0.9819 - val_loss: 0.0971 - val_accuracy: 0.9710\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.0662 - accuracy: 0.9823 - val_loss: 0.0959 - val_accuracy: 0.9720\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.0642 - accuracy: 0.9831 - val_loss: 0.0970 - val_accuracy: 0.9719\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.0620 - accuracy: 0.9839 - val_loss: 0.0918 - val_accuracy: 0.9733\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.0598 - accuracy: 0.9842 - val_loss: 0.0967 - val_accuracy: 0.9728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14003a390>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 10,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss is quite low at 0.0598, indicating the model is fitting the training data well.\n",
    "\n",
    "**The training accuracy is high at 0.9842,** so the model is predicting the training labels accurately.\n",
    "\n",
    "**The validation loss of 0.0967** is higher than the training loss. This gap indicates some overfitting is occurring.\n",
    "\n",
    "The model is still performing quite well on the **validation set with 0.9728 accuracy**, but the drop from training accuracy shows overfitting.\n",
    "\n",
    "Overall, the model seems to be fitting the training data very well, but needs some regularization or tuning to generalize better to the validation data and avoid overfitting.\n",
    "\n",
    "Some things you could try are:\n",
    "\n",
    "Adding dropout layers to regularize the model\n",
    "Trying a smaller model with less capacity to overfit\n",
    "Doing data augmentation to expand the training data\n",
    "Reducing training epochs to avoid over-optimizing on the training data\n",
    "So in summary, the model is overfitting which is expected, but some techniques could help improve validation performance while maintaining good training accuracy. The high training accuracy indicates low bias, so focusing on regularization for high variance would help.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 1, 'epochs': 20, 'steps': 363}\n",
      "[0, 1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.31771957874298096,\n",
       "  0.31946542859077454,\n",
       "  0.318808913230896,\n",
       "  0.3449883759021759,\n",
       "  0.32855361700057983,\n",
       "  0.32208824157714844],\n",
       " 'val_loss': [0.3402456045150757,\n",
       "  0.7006036043167114,\n",
       "  0.6078644394874573,\n",
       "  0.7966445088386536,\n",
       "  0.43870869278907776,\n",
       "  0.3419862985610962]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.epoch)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.2330968379974365,\n",
       "  0.512059211730957,\n",
       "  0.3954629600048065,\n",
       "  0.3456837236881256,\n",
       "  0.3154762387275696,\n",
       "  0.2936239540576935,\n",
       "  0.2761595845222473,\n",
       "  0.2615944445133209,\n",
       "  0.24905195832252502,\n",
       "  0.23805968463420868,\n",
       "  0.22810259461402893,\n",
       "  0.2190934419631958,\n",
       "  0.2106892168521881,\n",
       "  0.20303110778331757,\n",
       "  0.19597695767879486,\n",
       "  0.18938779830932617,\n",
       "  0.18302011489868164,\n",
       "  0.1774488091468811,\n",
       "  0.17158636450767517,\n",
       "  0.16640087962150574,\n",
       "  0.16150201857089996,\n",
       "  0.15699966251850128,\n",
       "  0.15225447714328766,\n",
       "  0.14814235270023346,\n",
       "  0.14401131868362427,\n",
       "  0.14034001529216766,\n",
       "  0.13661479949951172,\n",
       "  0.13305144011974335,\n",
       "  0.12980926036834717,\n",
       "  0.12659789621829987,\n",
       "  0.12353730201721191,\n",
       "  0.12059188634157181,\n",
       "  0.11785876005887985,\n",
       "  0.11503495275974274,\n",
       "  0.112479567527771,\n",
       "  0.10967199504375458,\n",
       "  0.10735592991113663,\n",
       "  0.1050608679652214,\n",
       "  0.10274162888526917,\n",
       "  0.10064034909009933,\n",
       "  0.098486989736557,\n",
       "  0.0965801253914833,\n",
       "  0.09461388736963272,\n",
       "  0.09267943352460861,\n",
       "  0.09076786041259766,\n",
       "  0.0889541283249855,\n",
       "  0.08725891262292862,\n",
       "  0.08560632169246674,\n",
       "  0.08397532999515533,\n",
       "  0.0824320912361145],\n",
       " 'accuracy': [0.6930199861526489,\n",
       "  0.868619978427887,\n",
       "  0.893339991569519,\n",
       "  0.9037399888038635,\n",
       "  0.9110400080680847,\n",
       "  0.9169800281524658,\n",
       "  0.9222400188446045,\n",
       "  0.9258999824523926,\n",
       "  0.9291999936103821,\n",
       "  0.9325600266456604,\n",
       "  0.9352200031280518,\n",
       "  0.9378399848937988,\n",
       "  0.9401999711990356,\n",
       "  0.9421399831771851,\n",
       "  0.9441999793052673,\n",
       "  0.9463199973106384,\n",
       "  0.9485800266265869,\n",
       "  0.9501000046730042,\n",
       "  0.9516400098800659,\n",
       "  0.952459990978241,\n",
       "  0.9543799757957458,\n",
       "  0.9556000232696533,\n",
       "  0.957360029220581,\n",
       "  0.9584199786186218,\n",
       "  0.9598000049591064,\n",
       "  0.9611200094223022,\n",
       "  0.961899995803833,\n",
       "  0.9630799889564514,\n",
       "  0.9638199806213379,\n",
       "  0.964680016040802,\n",
       "  0.9658399820327759,\n",
       "  0.9665200114250183,\n",
       "  0.9676399827003479,\n",
       "  0.9678400158882141,\n",
       "  0.9688000082969666,\n",
       "  0.9697999954223633,\n",
       "  0.9703400135040283,\n",
       "  0.9713199734687805,\n",
       "  0.9718400239944458,\n",
       "  0.9721800088882446,\n",
       "  0.9729200005531311,\n",
       "  0.973580002784729,\n",
       "  0.9738600254058838,\n",
       "  0.9745200276374817,\n",
       "  0.9751999974250793,\n",
       "  0.975820004940033,\n",
       "  0.9763399958610535,\n",
       "  0.9762600064277649,\n",
       "  0.9770399928092957,\n",
       "  0.977620005607605],\n",
       " 'val_loss': [0.5914443135261536,\n",
       "  0.39520153403282166,\n",
       "  0.332505464553833,\n",
       "  0.30009332299232483,\n",
       "  0.27843552827835083,\n",
       "  0.263681560754776,\n",
       "  0.24969731271266937,\n",
       "  0.23702318966388702,\n",
       "  0.22936950623989105,\n",
       "  0.2187252938747406,\n",
       "  0.2119191586971283,\n",
       "  0.20272135734558105,\n",
       "  0.19680768251419067,\n",
       "  0.19093357026576996,\n",
       "  0.18492338061332703,\n",
       "  0.17932982742786407,\n",
       "  0.17482824623584747,\n",
       "  0.17012520134449005,\n",
       "  0.16767367720603943,\n",
       "  0.1634354293346405,\n",
       "  0.15900343656539917,\n",
       "  0.15583060681819916,\n",
       "  0.15369486808776855,\n",
       "  0.14918050169944763,\n",
       "  0.14715634286403656,\n",
       "  0.14356693625450134,\n",
       "  0.1414368450641632,\n",
       "  0.1385086327791214,\n",
       "  0.13624441623687744,\n",
       "  0.13429665565490723,\n",
       "  0.13155600428581238,\n",
       "  0.1305442452430725,\n",
       "  0.12717792391777039,\n",
       "  0.125531405210495,\n",
       "  0.12433666735887527,\n",
       "  0.12543098628520966,\n",
       "  0.12275361269712448,\n",
       "  0.12004686146974564,\n",
       "  0.1186235323548317,\n",
       "  0.11751851439476013,\n",
       "  0.11633387207984924,\n",
       "  0.11314663290977478,\n",
       "  0.11257811635732651,\n",
       "  0.11113195866346359,\n",
       "  0.10997561365365982,\n",
       "  0.11039061099290848,\n",
       "  0.1089080348610878,\n",
       "  0.10632689297199249,\n",
       "  0.10531357675790787,\n",
       "  0.10515376925468445],\n",
       " 'val_accuracy': [0.8652999997138977,\n",
       "  0.8955000042915344,\n",
       "  0.909600019454956,\n",
       "  0.9178000092506409,\n",
       "  0.9222000241279602,\n",
       "  0.9251000285148621,\n",
       "  0.9282000064849854,\n",
       "  0.9343000054359436,\n",
       "  0.9355999827384949,\n",
       "  0.9386000037193298,\n",
       "  0.9409000277519226,\n",
       "  0.9453999996185303,\n",
       "  0.9473999738693237,\n",
       "  0.9484999775886536,\n",
       "  0.9510999917984009,\n",
       "  0.9516000151634216,\n",
       "  0.9521999955177307,\n",
       "  0.9538000226020813,\n",
       "  0.9546999931335449,\n",
       "  0.9556999802589417,\n",
       "  0.9556999802589417,\n",
       "  0.9573000073432922,\n",
       "  0.9581999778747559,\n",
       "  0.9581999778747559,\n",
       "  0.9584000110626221,\n",
       "  0.9613999724388123,\n",
       "  0.9616000056266785,\n",
       "  0.9617000222206116,\n",
       "  0.9627000093460083,\n",
       "  0.9632999897003174,\n",
       "  0.9634000062942505,\n",
       "  0.9646999835968018,\n",
       "  0.9643999934196472,\n",
       "  0.9660000205039978,\n",
       "  0.9659000039100647,\n",
       "  0.9660000205039978,\n",
       "  0.9661999940872192,\n",
       "  0.9672999978065491,\n",
       "  0.9677000045776367,\n",
       "  0.9679999947547913,\n",
       "  0.9690999984741211,\n",
       "  0.9672999978065491,\n",
       "  0.9685999751091003,\n",
       "  0.9692999720573425,\n",
       "  0.9690999984741211,\n",
       "  0.9688000082969666,\n",
       "  0.9685999751091003,\n",
       "  0.970300018787384,\n",
       "  0.9697999954223633,\n",
       "  0.9706000089645386]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.233097</td>\n",
       "      <td>0.69302</td>\n",
       "      <td>0.591444</td>\n",
       "      <td>0.8653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512059</td>\n",
       "      <td>0.86862</td>\n",
       "      <td>0.395202</td>\n",
       "      <td>0.8955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.395463</td>\n",
       "      <td>0.89334</td>\n",
       "      <td>0.332505</td>\n",
       "      <td>0.9096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.345684</td>\n",
       "      <td>0.90374</td>\n",
       "      <td>0.300093</td>\n",
       "      <td>0.9178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.315476</td>\n",
       "      <td>0.91104</td>\n",
       "      <td>0.278436</td>\n",
       "      <td>0.9222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.293624</td>\n",
       "      <td>0.91698</td>\n",
       "      <td>0.263682</td>\n",
       "      <td>0.9251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.276160</td>\n",
       "      <td>0.92224</td>\n",
       "      <td>0.249697</td>\n",
       "      <td>0.9282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.261594</td>\n",
       "      <td>0.92590</td>\n",
       "      <td>0.237023</td>\n",
       "      <td>0.9343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.249052</td>\n",
       "      <td>0.92920</td>\n",
       "      <td>0.229370</td>\n",
       "      <td>0.9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.238060</td>\n",
       "      <td>0.93256</td>\n",
       "      <td>0.218725</td>\n",
       "      <td>0.9386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.228103</td>\n",
       "      <td>0.93522</td>\n",
       "      <td>0.211919</td>\n",
       "      <td>0.9409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.219093</td>\n",
       "      <td>0.93784</td>\n",
       "      <td>0.202721</td>\n",
       "      <td>0.9454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.210689</td>\n",
       "      <td>0.94020</td>\n",
       "      <td>0.196808</td>\n",
       "      <td>0.9474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.203031</td>\n",
       "      <td>0.94214</td>\n",
       "      <td>0.190934</td>\n",
       "      <td>0.9485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.195977</td>\n",
       "      <td>0.94420</td>\n",
       "      <td>0.184923</td>\n",
       "      <td>0.9511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.189388</td>\n",
       "      <td>0.94632</td>\n",
       "      <td>0.179330</td>\n",
       "      <td>0.9516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.183020</td>\n",
       "      <td>0.94858</td>\n",
       "      <td>0.174828</td>\n",
       "      <td>0.9522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.95010</td>\n",
       "      <td>0.170125</td>\n",
       "      <td>0.9538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.171586</td>\n",
       "      <td>0.95164</td>\n",
       "      <td>0.167674</td>\n",
       "      <td>0.9547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.166401</td>\n",
       "      <td>0.95246</td>\n",
       "      <td>0.163435</td>\n",
       "      <td>0.9557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.161502</td>\n",
       "      <td>0.95438</td>\n",
       "      <td>0.159003</td>\n",
       "      <td>0.9557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.95560</td>\n",
       "      <td>0.155831</td>\n",
       "      <td>0.9573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.152254</td>\n",
       "      <td>0.95736</td>\n",
       "      <td>0.153695</td>\n",
       "      <td>0.9582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.148142</td>\n",
       "      <td>0.95842</td>\n",
       "      <td>0.149181</td>\n",
       "      <td>0.9582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.95980</td>\n",
       "      <td>0.147156</td>\n",
       "      <td>0.9584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.140340</td>\n",
       "      <td>0.96112</td>\n",
       "      <td>0.143567</td>\n",
       "      <td>0.9614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.136615</td>\n",
       "      <td>0.96190</td>\n",
       "      <td>0.141437</td>\n",
       "      <td>0.9616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.133051</td>\n",
       "      <td>0.96308</td>\n",
       "      <td>0.138509</td>\n",
       "      <td>0.9617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.129809</td>\n",
       "      <td>0.96382</td>\n",
       "      <td>0.136244</td>\n",
       "      <td>0.9627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.126598</td>\n",
       "      <td>0.96468</td>\n",
       "      <td>0.134297</td>\n",
       "      <td>0.9633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.123537</td>\n",
       "      <td>0.96584</td>\n",
       "      <td>0.131556</td>\n",
       "      <td>0.9634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.120592</td>\n",
       "      <td>0.96652</td>\n",
       "      <td>0.130544</td>\n",
       "      <td>0.9647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.117859</td>\n",
       "      <td>0.96764</td>\n",
       "      <td>0.127178</td>\n",
       "      <td>0.9644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.115035</td>\n",
       "      <td>0.96784</td>\n",
       "      <td>0.125531</td>\n",
       "      <td>0.9660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.112480</td>\n",
       "      <td>0.96880</td>\n",
       "      <td>0.124337</td>\n",
       "      <td>0.9659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.109672</td>\n",
       "      <td>0.96980</td>\n",
       "      <td>0.125431</td>\n",
       "      <td>0.9660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.107356</td>\n",
       "      <td>0.97034</td>\n",
       "      <td>0.122754</td>\n",
       "      <td>0.9662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.105061</td>\n",
       "      <td>0.97132</td>\n",
       "      <td>0.120047</td>\n",
       "      <td>0.9673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.102742</td>\n",
       "      <td>0.97184</td>\n",
       "      <td>0.118624</td>\n",
       "      <td>0.9677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.100640</td>\n",
       "      <td>0.97218</td>\n",
       "      <td>0.117519</td>\n",
       "      <td>0.9680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.098487</td>\n",
       "      <td>0.97292</td>\n",
       "      <td>0.116334</td>\n",
       "      <td>0.9691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.096580</td>\n",
       "      <td>0.97358</td>\n",
       "      <td>0.113147</td>\n",
       "      <td>0.9673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.094614</td>\n",
       "      <td>0.97386</td>\n",
       "      <td>0.112578</td>\n",
       "      <td>0.9686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.092679</td>\n",
       "      <td>0.97452</td>\n",
       "      <td>0.111132</td>\n",
       "      <td>0.9693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.090768</td>\n",
       "      <td>0.97520</td>\n",
       "      <td>0.109976</td>\n",
       "      <td>0.9691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.088954</td>\n",
       "      <td>0.97582</td>\n",
       "      <td>0.110391</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.087259</td>\n",
       "      <td>0.97634</td>\n",
       "      <td>0.108908</td>\n",
       "      <td>0.9686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.085606</td>\n",
       "      <td>0.97626</td>\n",
       "      <td>0.106327</td>\n",
       "      <td>0.9703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.083975</td>\n",
       "      <td>0.97704</td>\n",
       "      <td>0.105314</td>\n",
       "      <td>0.9698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.082432</td>\n",
       "      <td>0.97762</td>\n",
       "      <td>0.105154</td>\n",
       "      <td>0.9706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.233097   0.69302  0.591444        0.8653\n",
       "1   0.512059   0.86862  0.395202        0.8955\n",
       "2   0.395463   0.89334  0.332505        0.9096\n",
       "3   0.345684   0.90374  0.300093        0.9178\n",
       "4   0.315476   0.91104  0.278436        0.9222\n",
       "5   0.293624   0.91698  0.263682        0.9251\n",
       "6   0.276160   0.92224  0.249697        0.9282\n",
       "7   0.261594   0.92590  0.237023        0.9343\n",
       "8   0.249052   0.92920  0.229370        0.9356\n",
       "9   0.238060   0.93256  0.218725        0.9386\n",
       "10  0.228103   0.93522  0.211919        0.9409\n",
       "11  0.219093   0.93784  0.202721        0.9454\n",
       "12  0.210689   0.94020  0.196808        0.9474\n",
       "13  0.203031   0.94214  0.190934        0.9485\n",
       "14  0.195977   0.94420  0.184923        0.9511\n",
       "15  0.189388   0.94632  0.179330        0.9516\n",
       "16  0.183020   0.94858  0.174828        0.9522\n",
       "17  0.177449   0.95010  0.170125        0.9538\n",
       "18  0.171586   0.95164  0.167674        0.9547\n",
       "19  0.166401   0.95246  0.163435        0.9557\n",
       "20  0.161502   0.95438  0.159003        0.9557\n",
       "21  0.157000   0.95560  0.155831        0.9573\n",
       "22  0.152254   0.95736  0.153695        0.9582\n",
       "23  0.148142   0.95842  0.149181        0.9582\n",
       "24  0.144011   0.95980  0.147156        0.9584\n",
       "25  0.140340   0.96112  0.143567        0.9614\n",
       "26  0.136615   0.96190  0.141437        0.9616\n",
       "27  0.133051   0.96308  0.138509        0.9617\n",
       "28  0.129809   0.96382  0.136244        0.9627\n",
       "29  0.126598   0.96468  0.134297        0.9633\n",
       "30  0.123537   0.96584  0.131556        0.9634\n",
       "31  0.120592   0.96652  0.130544        0.9647\n",
       "32  0.117859   0.96764  0.127178        0.9644\n",
       "33  0.115035   0.96784  0.125531        0.9660\n",
       "34  0.112480   0.96880  0.124337        0.9659\n",
       "35  0.109672   0.96980  0.125431        0.9660\n",
       "36  0.107356   0.97034  0.122754        0.9662\n",
       "37  0.105061   0.97132  0.120047        0.9673\n",
       "38  0.102742   0.97184  0.118624        0.9677\n",
       "39  0.100640   0.97218  0.117519        0.9680\n",
       "40  0.098487   0.97292  0.116334        0.9691\n",
       "41  0.096580   0.97358  0.113147        0.9673\n",
       "42  0.094614   0.97386  0.112578        0.9686\n",
       "43  0.092679   0.97452  0.111132        0.9693\n",
       "44  0.090768   0.97520  0.109976        0.9691\n",
       "45  0.088954   0.97582  0.110391        0.9688\n",
       "46  0.087259   0.97634  0.108908        0.9686\n",
       "47  0.085606   0.97626  0.106327        0.9703\n",
       "48  0.083975   0.97704  0.105314        0.9698\n",
       "49  0.082432   0.97762  0.105154        0.9706"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0951 - accuracy: 0.9716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09512048959732056, 0.9715999960899353]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1473442946.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[40], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.imshow(X_test[0].reshape(28,28), cmap=plt.colormaps.get_cmap\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.colormaps.get_cmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.32941177, 0.7254902 , 0.62352943, 0.5921569 ,\n",
       "         0.23529412, 0.14117648, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.87058824, 0.99607843, 0.99607843, 0.99607843,\n",
       "         0.99607843, 0.94509804, 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.6666667 , 0.20392157, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2627451 , 0.44705883, 0.28235295, 0.44705883,\n",
       "         0.6392157 , 0.8901961 , 0.99607843, 0.88235295, 0.99607843,\n",
       "         0.99607843, 0.99607843, 0.98039216, 0.8980392 , 0.99607843,\n",
       "         0.99607843, 0.54901963, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "         0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "         0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.3254902 , 0.99215686,\n",
       "         0.81960785, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.08627451, 0.9137255 , 1.        ,\n",
       "         0.3254902 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5058824 , 0.99607843, 0.93333334,\n",
       "         0.17254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.23137255, 0.9764706 , 0.99607843, 0.24313726,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.03529412, 0.8039216 , 0.972549  , 0.22745098, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.49411765, 0.99607843, 0.7137255 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29411766,\n",
       "         0.9843137 , 0.9411765 , 0.22352941, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.07450981, 0.8666667 ,\n",
       "         0.99607843, 0.6509804 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "         0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14901961, 0.99607843, 0.99607843,\n",
       "         0.3019608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.12156863, 0.8784314 , 0.99607843, 0.4509804 ,\n",
       "         0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.23921569, 0.9490196 , 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.8117647 , 0.07058824, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 404ms/step\n",
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.002, 0.   , 0.   , 0.   , 0.998, 0.   ,\n",
       "        0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test[:1]).round(3)\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, ..., 6, 9, 9])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ColormapRegistry' object has no attribute 'get_cmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cm\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(X_test[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m), cmap\u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39mcolormaps\u001b[38;5;241m.\u001b[39mget_cmap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreys\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ColormapRegistry' object has no attribute 'get_cmap'"
     ]
    }
   ],
   "source": [
    "from matplotlib import cm\n",
    "plt.imshow(X_test[2].reshape(28,28), cmap=plt.colormaps.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from matplotlib import cm\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split the full housing dataset into training and test sets.\n",
    "\n",
    "Splits the features (X) and targets (y) into training and test \n",
    "sets for model training and evaluation.\n",
    "\"\"\"\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "\"\"\"Split the training set into training and validation sets.\n",
    "\n",
    "Further splits the features (X_train) and targets (y_train) from \n",
    "the full training set into training and validation sets for \n",
    "training, validation and hyperparameter tuning.\n",
    "\"\"\"\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362.8125"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11610/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.8953 - val_loss: 1.6512\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 2.0543 - val_loss: 0.4776\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4534 - val_loss: 0.4439\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.4191 - val_loss: 0.4647\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.3931\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3907 - val_loss: 0.3905\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3870 - val_loss: 0.3800\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3817 - val_loss: 0.3728\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3795 - val_loss: 0.3723\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3746 - val_loss: 0.3731\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3745 - val_loss: 0.5009\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3828 - val_loss: 0.3673\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3718 - val_loss: 0.3678\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3771 - val_loss: 0.5934\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3727 - val_loss: 0.3666\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3645 - val_loss: 0.3571\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3628 - val_loss: 0.3581\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3619 - val_loss: 0.3553\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3642 - val_loss: 0.3590\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3627 - val_loss: 0.3566\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Creates a sequential Keras model with 2 dense layers.\n",
    "\n",
    "The first layer has 30 units and a 'relu' activation, \n",
    "with an input shape matching the training data.\n",
    "\n",
    "The second layer has 1 unit for regression.\"\"\"\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu',\n",
    "                      input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\"\"\"Compiles the model by specifying the loss function \n",
    "and optimizer.\n",
    "\n",
    "Parameters:\n",
    "- loss: Loss function to optimize during training. \n",
    "- optimizer: Optimization algorithm for updating weights.\n",
    "\"\"\"\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "             optimizer = \"sgd\")\n",
    "\n",
    "\"\"\"Fits the Keras model on the training data.\n",
    "\n",
    "Arguments:\n",
    "- X_train: Training data input features.  \n",
    "- y_train: Training data target values.\n",
    "- epochs: Number of epochs to train for.\n",
    "- validation_data: Validation data to evaluate loss and metrics.\n",
    "\"\"\"\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs = 20,\n",
    "                   validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*30 + 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 30)                270       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301 (1.18 KB)\n",
      "Trainable params: 301 (1.18 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/162 [====>.........................] - ETA: 0s - loss: 0.3119"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 1s 8ms/step - loss: 0.3674\n",
      "MSE: 0.3674147129058838\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluates the MSE loss on the test data.\n",
    "\"\"\"\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(\"MSE:\", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 180ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.4448686],\n",
       "       [1.6384043],\n",
       "       [2.7036328],\n",
       "       [1.1734333],\n",
       "       [4.5801682]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Predict class labels for the first 5 samples in X_test\n",
    "y_pred = model.predict(X_test[:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_keras_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_keras_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_keras_model\") # save the model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter: Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "358/363 [============================>.] - ETA: 0s - loss: 0.3440INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 4s 8ms/step - loss: 0.3450\n",
      "Epoch 2/30\n",
      "360/363 [============================>.] - ETA: 0s - loss: 0.3441INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3439\n",
      "Epoch 3/30\n",
      "362/363 [============================>.] - ETA: 0s - loss: 0.3421INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3418\n",
      "Epoch 4/30\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3481INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3480\n",
      "Epoch 5/30\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.3400INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 4s 12ms/step - loss: 0.3419\n",
      "Epoch 6/30\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4331INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 9ms/step - loss: 0.4340\n",
      "Epoch 7/30\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.3775INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3776\n",
      "Epoch 8/30\n",
      "360/363 [============================>.] - ETA: 0s - loss: 0.3611INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3607\n",
      "Epoch 9/30\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.3523INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3524\n",
      "Epoch 10/30\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3467INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3461\n",
      "Epoch 11/30\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.3428INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3422\n",
      "Epoch 12/30\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.3407INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 4s 11ms/step - loss: 0.3401\n",
      "Epoch 13/30\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.3545INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 4s 12ms/step - loss: 0.3527\n",
      "Epoch 14/30\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3374INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 5s 13ms/step - loss: 0.3375\n",
      "Epoch 15/30\n",
      "360/363 [============================>.] - ETA: 0s - loss: 0.3342INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3339\n",
      "Epoch 16/30\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3319INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3319\n",
      "Epoch 17/30\n",
      "360/363 [============================>.] - ETA: 0s - loss: 0.3302INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 4s 10ms/step - loss: 0.3319\n",
      "Epoch 18/30\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.3337INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3327\n",
      "Epoch 19/30\n",
      "362/363 [============================>.] - ETA: 0s - loss: 0.3282INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3283\n",
      "Epoch 20/30\n",
      "350/363 [===========================>..] - ETA: 0s - loss: 0.3267INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3257\n",
      "Epoch 21/30\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3251INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3245\n",
      "Epoch 22/30\n",
      "358/363 [============================>.] - ETA: 0s - loss: 0.3245INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3248\n",
      "Epoch 23/30\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.3429INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3434\n",
      "Epoch 24/30\n",
      "347/363 [===========================>..] - ETA: 0s - loss: 0.3210INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3228\n",
      "Epoch 25/30\n",
      "362/363 [============================>.] - ETA: 0s - loss: 0.3206INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3207\n",
      "Epoch 26/30\n",
      "361/363 [============================>.] - ETA: 0s - loss: 0.3378INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3381\n",
      "Epoch 27/30\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.3238INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3221\n",
      "Epoch 28/30\n",
      "347/363 [===========================>..] - ETA: 0s - loss: 0.3211INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3213\n",
      "Epoch 29/30\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.3189INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3190\n",
      "Epoch 30/30\n",
      "361/363 [============================>.] - ETA: 0s - loss: 0.3184INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: callback_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3178\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint\n",
    "\"\"\"\n",
    "Fits the model on the training data for a given number of epochs, \n",
    "saving the best checkpoint to a callback.\n",
    "\"\"\"\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=30,\n",
    "                   callbacks = [checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3177 - val_loss: 0.3402\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3195 - val_loss: 0.7006\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3188 - val_loss: 0.6079\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3450 - val_loss: 0.7966\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3286 - val_loss: 0.4387\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3221 - val_loss: 0.3420\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "early_stopping_cb: Callback to stop training early if validation loss stops improving.\n",
    "\n",
    "patience: Number of epochs with no improvement after which training will be stopped.\n",
    "\n",
    "history: Training history returned by model.fit.\n",
    "\n",
    "X_train, y_train: Training data and labels. \n",
    "\n",
    "epochs: Number of training epochs.\n",
    "\n",
    "validation_data: Validation data tuple (X_valid, y_valid).\n",
    "\n",
    "Fit the model on the training data for a given number of epochs, validating after each epoch. \n",
    "Stop early if validation loss doesn't improve for patience epochs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=20,\n",
    "                    validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
