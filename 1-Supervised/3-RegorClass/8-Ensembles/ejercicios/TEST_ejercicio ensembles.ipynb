{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios ensembling\n",
    "En este ejercicio vas a realizar prediciones sobre un dataset de ciudadanos indios diabéticos. Se trata de un problema de clasificación en el que intentaremos predecir 1 (diabético) 0 (no diabético). Todas las variables son numércias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga las librerias que consideres comunes al notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lee los datos de [esta direccion](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)\n",
    "Los nombres de columnas son:\n",
    "```Python\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0       6   148    72    35     0  33.6  0.627   50      1\n",
       "1       1    85    66    29     0  26.6  0.351   31      0\n",
       "2       8   183    64     0     0  23.3  0.672   32      1\n",
       "3       1    89    66    23    94  28.1  0.167   21      0\n",
       "4       0   137    40    35   168  43.1  2.288   33      1\n",
       "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
       "763    10   101    76    48   180  32.9  0.171   63      0\n",
       "764     2   122    70    27     0  36.8  0.340   27      0\n",
       "765     5   121    72    23   112  26.2  0.245   30      0\n",
       "766     1   126    60     0     0  30.1  0.349   47      1\n",
       "767     1    93    70    31     0  30.4  0.315   23      0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = pd.read_csv(url, names=names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bagging\n",
    "Para este apartado tendrás que crear un ensemble utilizando la técnica de bagging ([BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)), mediante la cual combinarás 100 [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Recuerda utilizar también [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) con 10 kfolds.\n",
    "\n",
    "**Para este apartado y siguientes, no hace falta que dividas en train/test**, por hacerlo más sencillo. Simplemente divide tus datos en features y target.\n",
    "\n",
    "Establece una semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7720437457279563"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results_bagg = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "results_bagg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest\n",
    "En este caso entrena un [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) con 100 árboles y un `max_features` de 3. También con validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775974025974026"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results_rf = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "results_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AdaBoost\n",
    "Implementa un [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) con 30 árboles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.760457963089542"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "num_trees = 30\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results_ada = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "results_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A result of 0.76 for ada when using AdaBoostClassifier indicates that the cross-validated accuracy of the AdaBoostClassifier model on the given data is 76%.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "AdaBoostClassifier is an ensemble method that combines multiple weak decision tree classifiers into a strong classifier.\n",
    "\n",
    "The model was trained on the provided input data (X) and labels (Y).\n",
    "\n",
    "Cross-validation was used to evaluate the model's accuracy. The data was split into 10 folds, the model was trained on 9 folds and tested on the remaining fold. This was repeated for each fold.\n",
    "\n",
    "The accuracy of the model was calculated on each fold. These 10 accuracy values were then averaged to produce the overall cross-validated accuracy score.\n",
    "\n",
    "This final cross-validated accuracy score was 0.76 or 76%.\n",
    "\n",
    "So in summary, a score of 0.76 means the AdaBoostClassifier was able to correctly classify, on average, 76% of the examples in a held-out fold, when evaluated across the 10 folds of cross-validation.\n",
    "\n",
    "This gives a realistic estimate of the model's generalization performance on new unseen data. A score of 0.76 is reasonably good for many classification problems. But whether it is acceptable or not depends on the specific use case and requirements. Overall, the cross-validated accuracy gives a reliable metric to evaluate and tune the AdaBoostClassifier for a given dataset.\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GradientBoosting\n",
    "Implementa un [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) con 100 estimadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes in two main inputs:\n",
    "\n",
    "n_estimators - This specifies the number of decision trees to use in the ensemble model. It is set to the trees variable, which we can assume contains the desired number of trees.\n",
    "\n",
    "random_state - This sets the random seed for reproducibility.\n",
    "\n",
    "The output is a configured GradientBoostingClassifier model object that can be used for training and making predictions.\n",
    "\n",
    "The GradientBoostingClassifier model works by combining the predictions from multiple decision tree models. It trains the decision trees sequentially, each one learning from the errors of the previous tree. Setting a higher n_estimators results in more trees being used, which can improve accuracy at the cost of increased training time.\n",
    "\n",
    "The random_state parameter seeds the random number generator used while training the trees. This ensures repeatable results across multiple runs with the same data.\n",
    "\n",
    "By instantiating the GradientBoostingClassifier with these parameters, the code configures an ensemble model ready for training on data. The trained model can then be used to make predictions on new data by combining the predictions of its constituent decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7681989063568012"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "num_trees = 100\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results_gb = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "results_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. XGBoost\n",
    "Para este apartado utiliza un [XGBoostClassifier](https://docs.getml.com/latest/api/getml.predictors.XGBoostClassifier.html) con 100 estimadores. XGBoost no forma parte de la suite de modelos de sklearn, por lo que tendrás que instalarlo con pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier\n",
    "\n",
    "The XGBClassifier is a classifier model that is part of the XGBoost library. It is used for building gradient boosted decision tree models for classification tasks.\n",
    "\n",
    "It takes as input the hyperparameters for configuring the model such as the number of estimators (n_estimators) which is the number of trees to build in the ensemble model. It also takes the training data features (X) and labels (y) to fit the model.\n",
    "\n",
    "The XGBClassifier is initialized by specifying the model hyperparameters like n_estimators. The .fit() method is used to train the model on the provided training data. This builds an ensemble of decision trees on the training data.\n",
    "\n",
    "The trained model can then be used to make predictions on new unseen data using the .predict() method. This will output the predicted class labels.\n",
    "\n",
    "Some key aspects of XGBClassifier:\n",
    "\n",
    "Ensemble method that combines multiple decision trees to improve accuracy.\n",
    "\n",
    "Uses gradient boosting to incrementally build trees that focus on hard to classify examples.\n",
    "\n",
    "Handles sparse data well and provides good accuracy with default parameters.\n",
    "\n",
    "Fast prediction speed and high performance compared to other tree models.\n",
    "\n",
    "Regularization helps prevent overfitting the training data.\n",
    "\n",
    "So in summary, it is a gradient boosted decision tree classifier that builds an ensemble model useful for classification tasks and can achieve high accuracy without too much tuning.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7395591250854407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "y = df['class'] \n",
    "X = df.drop('class', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "y = df['class'] \n",
    "X = df.drop('class', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_grid = {'C': [0.1, 1, 10, 100], \n",
    "              'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "# Create grid search object\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "\n",
    "# Fit on training data\n",
    "grid.fit(X_train, y_train) \n",
    "\n",
    "# View best hyperparameters \n",
    "print(grid.best_params_)\n",
    "\n",
    "# Build model with best hyperparameters\n",
    "model = SVC(C=grid.best_params_['C'], \n",
    "            gamma=grid.best_params_['gamma'])\n",
    "\n",
    "# Refit on full training set\n",
    "model.fit(X_train, y_train)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "model = XGBClassifier(n_estimators=100)\n",
    "results_xgb = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "print(results_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resultados\n",
    "Crea un series con los resultados y sus algoritmos, ordenándolos de mayor a menor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest       0.775974\n",
       "Bagging DT          0.772044\n",
       "GradientBoosting    0.768199\n",
       "Ada Boost           0.760458\n",
       "XGBoost             0.739559\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resul = [results_bagg, results_rf, results_ada, results_gb, results_xgb]\n",
    "algori = [\"Bagging DT\", \"Random Forest\", \"Ada Boost\", \"GradientBoosting\", \"XGBoost\"]\n",
    "\n",
    "resultados = pd.Series(resul, algori).sort_values(ascending=False)\n",
    "resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
