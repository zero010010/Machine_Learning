{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios ensembling\n",
    "En este ejercicio vas a realizar prediciones sobre un dataset de ciudadanos indios diabéticos. Se trata de un problema de clasificación en el que intentaremos predecir 1 (diabético) 0 (no diabético). Todas las variables son numércias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Carga las librerias que consideres comunes al notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lee los datos de [esta direccion](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)\n",
    "Los nombres de columnas son:\n",
    "```Python\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0       6   148    72    35     0  33.6  0.627   50      1\n",
       "1       1    85    66    29     0  26.6  0.351   31      0\n",
       "2       8   183    64     0     0  23.3  0.672   32      1\n",
       "3       1    89    66    23    94  28.1  0.167   21      0\n",
       "4       0   137    40    35   168  43.1  2.288   33      1\n",
       "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
       "763    10   101    76    48   180  32.9  0.171   63      0\n",
       "764     2   122    70    27     0  36.8  0.340   27      0\n",
       "765     5   121    72    23   112  26.2  0.245   30      0\n",
       "766     1   126    60     0     0  30.1  0.349   47      1\n",
       "767     1    93    70    31     0  30.4  0.315   23      0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = pd.read_csv(url, names=names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bagging\n",
    "Para este apartado tendrás que crear un ensemble utilizando la técnica de bagging ([BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)), mediante la cual combinarás 100 [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Recuerda utilizar también [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) con 10 kfolds.\n",
    "\n",
    "**Para este apartado y siguientes, no hace falta que dividas en train/test**, por hacerlo más sencillo. Simplemente divide tus datos en features y target.\n",
    "\n",
    "Establece una semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "array=df.values\n",
    "X =array[:,0:8]\n",
    "Y = array[:,8]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KFold cross-validation allows you to train and test your model multiple times on different subsets of the data. This helps reduce variability and gives a more reliable estimate of model performance.\n",
    "\n",
    "* It creates the object that will handle splitting the data into 10 folds for cross-validation when evaluating the model later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775974025974026"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = model_selection.KFold(n_splits=10)\n",
    "dtc = DecisionTreeClassifier()\n",
    "num_trees = 100 #bagging classifier 100 decision tree models.\n",
    "\n",
    "model = BaggingClassifier(base_estimator=dtc, n_estimators=num_trees, random_state=seed)\n",
    "\n",
    "Bagging = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest\n",
    "En este caso entrena un [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) con 100 árboles y un `max_features` de 3. También con validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7707792207792208"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "trees = 100\n",
    "max_features = 3\n",
    "\n",
    "# Split data into k folds for cross-validation and initialize random forest classifier\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "model = RandomForestClassifier(n_estimators=trees, max_features=max_features)\n",
    "\n",
    "\n",
    "# Cross-validate the random forest classifier model using k-fold \n",
    "# Calculate the mean cross-validation score.\n",
    "rf = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. AdaBoost\n",
    "Implementa un [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) con 30 árboles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.760457963089542"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "trees = 30\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "\n",
    "# Initializes an AdaBoostClassifier with the given number of estimators\n",
    "# and a fixed random state for reproducibility.\n",
    "model = AdaBoostClassifier(n_estimators=trees, random_state=seed)\n",
    "# mean accuracy score across the 10 folds of cross-validation.\n",
    "ada = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Adaboost trains the model using cross-validation, evaluates the accuracy on each fold, and averages the folds to produce a cross-validated accuracy score. \n",
    " * This gives an estimate of how well the model will generalize to new data. The cross-validation helps reduce overfitting and gives a more realistic evaluation of model performance.\n",
    " *  The code provides a way to evaluate and tune the AdaBoostClassifier using this cross-validated metric.\n",
    "\n",
    " * **Score of 0.76** means the AdaBoostClassifier was able to correctly classify, on average, 76% of the examples in a held-out fold, when evaluated across the 10 folds of cross-validation.\n",
    "\n",
    "This gives a realistic estimate of the model's generalization performance on new unseen data. A score of 0.76 is reasonably good for many classification problems. But whether it is acceptable or not depends on the specific use case and requirements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. GradientBoosting\n",
    "Implementa un [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) con 100 estimadores\n",
    "\n",
    "The GradientBoostingClassifier model works by combining the predictions from multiple decision tree models. It trains the decision trees sequentially, each one learning from the errors of the previous tree. Setting a higher n_estimators results in more trees being used, which can improve accuracy at the cost of increased training time.\n",
    "\n",
    "The random_state parameter seeds the random number generator used while training the trees. This ensures repeatable results across multiple runs with the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642857142857143"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trees = 100\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "model = GradientBoostingClassifier(n_estimators=trees, random_state=seed)\n",
    "gbc = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "gbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. XGBoost\n",
    "Para este apartado utiliza un [XGBoostClassifier](https://docs.getml.com/latest/api/getml.predictors.XGBoostClassifier.html) con 100 estimadores. XGBoost no forma parte de la suite de modelos de sklearn, por lo que tendrás que instalarlo con pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key aspects of XGBClassifier:\n",
    "\n",
    "Ensemble method that combines multiple decision trees to improve accuracy.\n",
    "\n",
    "Uses gradient boosting to incrementally build trees that focus on hard to classify examples.\n",
    "\n",
    "Handles sparse data well and provides good accuracy with default parameters.\n",
    "\n",
    "Fast prediction speed and high performance compared to other tree models.\n",
    "\n",
    "Regularization helps prevent overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7421736158578265"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "model = XGBClassifier(n_estimators=100)\n",
    "xgb = model_selection.cross_val_score(model, X, Y, cv=kfold).mean()\n",
    "xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Primeros resultados\n",
    "Crea un dataframe con los resultados y sus algoritmos, ordenándolos de mayor a menor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bagging DT          0.775974\n",
       "Random Forest       0.770779\n",
       "GradientBoosting    0.764286\n",
       "Ada Boost           0.760458\n",
       "XGBoost             0.742174\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [Bagging, rf, ada, gbc, xgb]\n",
    "models = [\"Bagging DT\", \"Random Forest\", \"Ada Boost\", \"GradientBoosting\", \"XGBoost\"]\n",
    "\n",
    "resume = pd.Series(result, models).sort_values(ascending=False)\n",
    "resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Hiperparametrización\n",
    "Vuelve a entrenar los modelos de nuevo, pero esta vez dividiendo el conjunto de datos en train/test y utilizando un gridsearch para encontrar los mejores hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "# Split data into X and y\n",
    "y = df['class'] \n",
    "X = df.drop('class', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Random Forest accuracy: 72.73%\n"
     ]
    }
   ],
   "source": [
    "# opt random forest \n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [5, 8, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create grid search object\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(), \n",
    "    param_grid=param_grid,\n",
    "    cv=5, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "rf_grid.fit(X_train, y_train)  \n",
    "\n",
    "# Extract best model\n",
    "best_model = rf_grid.best_estimator_\n",
    "\n",
    "# Evaluate on test data \n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Optimized Random Forest accuracy: {:.2f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Bagging:  0.7498803827751196\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# KFold cross-validation\n",
    "kfold = KFold(n_splits=10) \n",
    "\n",
    "# Base model\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "# Bagging Classifier\n",
    "model = BaggingClassifier(base_estimator=dtc)\n",
    "\n",
    "# Hyperparameters to tune\n",
    "parameters = {'n_estimators': [10, 50, 100], \n",
    "              'max_samples': [0.5, 1.0]}\n",
    "\n",
    "# Grid Search \n",
    "clf = GridSearchCV(model, parameters, cv=kfold)\n",
    "\n",
    "# Fit grid search\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Best model\n",
    "best_model = clf.best_estimator_\n",
    "\n",
    "# Evaluate model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(best_model, X, y, cv=kfold)\n",
    "\n",
    "print('Accuracy Bagging: ', scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ADA: 0.7669002050580999\n"
     ]
    }
   ],
   "source": [
    "# opt AdBoost\n",
    "\n",
    "# 10-fold cross validation\n",
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "# AdaBoost classifier\n",
    "model = AdaBoostClassifier() \n",
    "\n",
    "# Hyperparameters to tune  \n",
    "parameters = {'n_estimators': [50, 100, 200],\n",
    "              'learning_rate': [0.1, 0.5, 1]}\n",
    "\n",
    "# Grid Search\n",
    "clf = GridSearchCV(model, parameters, cv=kfold)\n",
    "\n",
    "# Fit grid search   \n",
    "clf.fit(X, y)\n",
    "\n",
    "# Best model\n",
    "best_model = clf.best_estimator_\n",
    "\n",
    "# Evaluate model\n",
    "scores = cross_val_score(best_model, X, y, cv=kfold)\n",
    "print('Accuracy ADA:', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy gbc: 0.7655673274094327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# opt GradientBoostingClassifier\n",
    "# 100 estimators\n",
    "trees = 100\n",
    "\n",
    "# 10-fold cross validation\n",
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "# Gradient Boosting Classifier \n",
    "model = GradientBoostingClassifier(n_estimators=trees)\n",
    "\n",
    "# Parameters to tune\n",
    "parameters = {'max_depth': [3, 5, 8], \n",
    "              'learning_rate': [0.1, 0.5, 1],\n",
    "              'max_features': [0.5, 1.0]}\n",
    "\n",
    "# Grid Search \n",
    "clf = GridSearchCV(model, parameters, cv=kfold)\n",
    "\n",
    "# Fit grid search\n",
    "clf.fit(X, y)  \n",
    "\n",
    "# Best model\n",
    "best_model = clf.best_estimator_\n",
    "\n",
    "# Evaluate model\n",
    "scores = cross_val_score(best_model, X, y, cv=kfold)\n",
    "print('Accuracy gbc:', scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy xgb: 0.7655331510594668\n"
     ]
    }
   ],
   "source": [
    "# opt xgboost\n",
    "\n",
    "# 10-fold cross validation\n",
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "# XGBoost classifier\n",
    "model = XGBClassifier(n_estimators=100) \n",
    "\n",
    "# Parameters to tune\n",
    "parameters = {'max_depth': [3, 5, 8],  \n",
    "              'learning_rate': [0.1, 0.5, 1], \n",
    "              'n_estimators': [100, 500, 1000]}\n",
    "\n",
    "# Grid Search\n",
    "clf = GridSearchCV(model, parameters, cv=kfold)\n",
    "\n",
    "# Fit grid search\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Best model  \n",
    "best_model = clf.best_estimator_\n",
    "\n",
    "# Evaluate model\n",
    "scores = cross_val_score(best_model, X, y, cv=kfold)\n",
    "print('Accuracy xgb:', scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Conclusiones finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensemble models like random forests and gradient boosting tend to outperform individual models like logistic regression and SVM. This is because they combine multiple weaker models to create a stronger overall model.\n",
    "\n",
    "* Tuning ensemble models using grid search improves their performance by finding optimal hyperparameters like n_estimators, max_depth etc. The improvements can be significant.\n",
    "\n",
    "* Using cross-validation allows more reliable evaluation of model performance by testing on multiple held-out subsets of the training data.\n",
    "\n",
    "* Ensemble methods can overfit if the individual models are too complex or trained for too long. Tuning and cross-validation helps prevent overfitting.\n",
    "\n",
    "* Ensembles like random forest naturally do feature selection by choosing the most predictive features. This results in further performance gains.\n",
    "\n",
    "* Overall, ensembles provide a big boost in model performance. With proper tuning and evaluation, they achieve high accuracy while preventing overfitting. Stacking further models can provide additional gains.\n",
    "\n",
    "* Ensembles are a powerful technique that should be part of any machine learning practitioner's toolkit. Proper usage as demonstrated in this notebook can enhance most modeling pipelines.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec2a379ed5c25334a484232182c9d38ef8bd9861e2542d0c517568c4f99a9a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
